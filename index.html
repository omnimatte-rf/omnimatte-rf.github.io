<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="preload" as="font" href="/_next/static/media/e7c7dbb62ddcf6fa-s.p.woff2" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/4dc903766d1a074c.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/5baff23b1771b846.css" data-precedence="next"/><title>OmnimatteRF: Robust Omnimatte with 3D Background Modeling</title><meta name="description" content="OmnimatteRF: Robust Omnimatte with 3D Background Modeling"/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="__className_f9cc9d"><main><section class="component_title__UkgDO"><h1><strong>OmnimatteRF</strong><br/>Robust Omnimatte with 3D Background Modeling</h1><span class="component_published__rjSpb">ICCV 2023</span></section><section class="component_authorList__sLmCk"><span><a href="https://scholar.google.com/citations?user=2Vh_sboAAAAJ&amp;hl=en" target="_blank">Geng Lin</a><sup>1</sup></span><span><a href="http://chengao.vision/" target="_blank">Chen Gao</a><sup>2</sup></span><span><a href="https://jbhuang0604.github.io/" target="_blank">Jia-Bin Huang</a><sup>1,2</sup></span><span><a href="https://changilkim.com/" target="_blank">Changil Kim</a><sup>2</sup></span><span><a href="https://www.linkedin.com/in/yipeng-wang99/" target="_blank">Yipeng Wang</a><sup>2</sup></span><span><a href="https://www.cs.umd.edu/~zwicker/" target="_blank">Matthias Zwicker</a><sup>1</sup></span><span><a href="https://scholar.google.com/citations?user=bluhHm8AAAAJ&amp;hl=en" target="_blank">Ayush Saraf</a><sup>2</sup></span></section><section class="component_affiliationList__i5VyJ"><span><sup>1</sup>University of Maryland, College Park</span><span><sup>2</sup>Meta</span></section><section class="component_links__O6lLr"><a class="component_link__LvP1v" target="_blank" href="https://arxiv.org/abs/2309.07749"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 00-3.375-3.375h-1.5A1.125 1.125 0 0113.5 7.125v-1.5a3.375 3.375 0 00-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 00-9-9z"></path></svg> Paper</a><a class="component_link__LvP1v" target="_blank" href="https://github.com/facebookresearch/OmnimatteRF"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true"><path stroke-linecap="round" stroke-linejoin="round" d="M17.25 6.75L22.5 12l-5.25 5.25m-10.5 0L1.5 12l5.25-5.25m7.5-3l-4.5 16.5"></path></svg> Code</a><a class="component_link__LvP1v" href="javascript:"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true"><path stroke-linecap="round" stroke-linejoin="round" d="M6.429 9.75L2.25 12l4.179 2.25m0-4.5l5.571 3 5.571-3m-11.142 0L2.25 7.5 12 2.25l9.75 5.25-4.179 2.25m0 0L21.75 12l-4.179 2.25m0 0l4.179 2.25L12 21.75 2.25 16.5l4.179-2.25m11.142 0l-5.571 3-5.571-3"></path></svg> Data</a></section><section class="empty"></section><section><center><video src="/matting-teaser.mp4" autoPlay="" loop="" muted=""></video></center></section><section><div class="carousel-root"><div class="carousel carousel-slider" style="width:100%"><button type="button" aria-label="previous slide / item" class="control-arrow control-prev control-disabled"></button><div class="slider-wrapper axis-horizontal"><ul class="slider animated" style="-webkit-transform:translate3d(-100%,0,0);-ms-transform:translate3d(-100%,0,0);-o-transform:translate3d(-100%,0,0);transform:translate3d(-100%,0,0);-webkit-transition-duration:350ms;-moz-transition-duration:350ms;-o-transition-duration:350ms;transition-duration:350ms;-ms-transition-duration:350ms"><li class="slide"><div><video id="demo-video-5" src="/demo/demo6.mp4" width="100%" autoPlay="" loop="" muted="" controls=""></video></div></li><li class="slide selected previous"><div><video id="demo-video-0" src="/demo/demo1.mp4" width="100%" autoPlay="" loop="" muted="" controls=""></video></div></li><li class="slide"><div><video id="demo-video-1" src="/demo/demo2.mp4" width="100%" autoPlay="" loop="" muted="" controls=""></video></div></li><li class="slide"><div><video id="demo-video-2" src="/demo/demo3.mp4" width="100%" autoPlay="" loop="" muted="" controls=""></video></div></li><li class="slide"><div><video id="demo-video-3" src="/demo/demo4.mp4" width="100%" autoPlay="" loop="" muted="" controls=""></video></div></li><li class="slide"><div><video id="demo-video-4" src="/demo/demo5.mp4" width="100%" autoPlay="" loop="" muted="" controls=""></video></div></li><li class="slide"><div><video id="demo-video-5" src="/demo/demo6.mp4" width="100%" autoPlay="" loop="" muted="" controls=""></video></div></li><li class="slide selected previous"><div><video id="demo-video-0" src="/demo/demo1.mp4" width="100%" autoPlay="" loop="" muted="" controls=""></video></div></li></ul></div><button type="button" aria-label="next slide / item" class="control-arrow control-next control-disabled"></button></div><div class="carousel"><div class="thumbs-wrapper axis-vertical"><button type="button" class="control-arrow control-prev control-disabled" aria-label="previous slide / item"></button><ul class="thumbs animated" style="-webkit-transform:translate3d(0,0,0);-moz-transform:translate3d(0,0,0);-ms-transform:translate3d(0,0,0);-o-transform:translate3d(0,0,0);transform:translate3d(0,0,0);-ms-transform:translate3d(0,0,0);-webkit-transition-duration:350ms;-moz-transition-duration:350ms;-ms-transition-duration:350ms;-o-transition-duration:350ms;transition-duration:350ms;-ms-transition-duration:350ms"><li class="thumb selected" aria-label="slide item 1" style="width:120px" role="button" tabindex="0"><img src="/demo/demo1.jpg"/></li><li class="thumb" aria-label="slide item 2" style="width:120px" role="button" tabindex="0"><img src="/demo/demo2.jpg"/></li><li class="thumb" aria-label="slide item 3" style="width:120px" role="button" tabindex="0"><img src="/demo/demo3.jpg"/></li><li class="thumb" aria-label="slide item 4" style="width:120px" role="button" tabindex="0"><img src="/demo/demo4.jpg"/></li><li class="thumb" aria-label="slide item 5" style="width:120px" role="button" tabindex="0"><img src="/demo/demo5.jpg"/></li><li class="thumb" aria-label="slide item 6" style="width:120px" role="button" tabindex="0"><img src="/demo/demo6.jpg"/></li></ul><button type="button" class="control-arrow control-next control-disabled" aria-label="next slide / item"></button></div></div></div><p>We present OmnimatteRF, which creates mattes with associated effects like shadows from in-the-wild videos with coarse masks.</p></section><section><h2>Abstract</h2><p> Video matting has broad applications, from adding interesting effects to casually captured movies to assisting video production professionals. Matting with associated effects like shadows and reflections has also attracted increasing research activity, and methods like Omnimatte have been proposed to separate foreground objects of interest into their own layers. However, prior works represent video backgrounds as 2D image layers, limiting their capacity to express more complicated scenes, thus hindering application to real-world videos. In this paper, we propose a novel video matting method, F2B3, that combines 2D foreground layers and a 3D background model. The 2D layers preserve the details of the subjects, while the 3D background robustly reconstructs scenes in real-world videos. Extensive experiments demonstrate that our method reconstructs with better quality on various videos.</p></section><section class="component_method__8zQ7M"><h2>Method</h2><center><img src="/overview.jpg"/></center><p></p><p>OmnimatteRF extends <i>Omnimatting</i> to a larger variety of real-world videos with a combination of 2D foreground layers and a background radiance field.</p></section><section class="component_datasets__fjKYl"><h2 id="data">Data &amp; Results</h2><section class="VideosView_videos__nuGCy"><span>Choose a dataset and video to view input with coarse masks.</span><section class="VideosView_groupSelect__0BnyL"><button class="VideosView_groupCurrent__cCmk5">Movies</button><button>Wild</button><button>Kubrics</button><button>Davis</button><button>Others</button></section><section class="VideosView_videoSelect__DbaUe"><div class="VideosView_videoPreview__J_K5w VideosView_videoCurrent__cyrzr"><a href="javascript:"><video src="/preview/Movies/chicken.mp4" autoPlay="" loop="" muted=""></video></a><span>chicken</span></div><div class="VideosView_videoPreview__J_K5w"><a href="javascript:"><video src="/preview/Movies/dodge.mp4" autoPlay="" loop="" muted=""></video></a><span>dodge</span></div><div class="VideosView_videoPreview__J_K5w"><a href="javascript:"><video src="/preview/Movies/dog.mp4" autoPlay="" loop="" muted=""></video></a><span>dog</span></div><div class="VideosView_videoPreview__J_K5w"><a href="javascript:"><video src="/preview/Movies/donkey.mp4" autoPlay="" loop="" muted=""></video></a><span>donkey</span></div><div class="VideosView_videoPreview__J_K5w"><a href="javascript:"><video src="/preview/Movies/rooster.mp4" autoPlay="" loop="" muted=""></video></a><span>rooster</span></div></section><section><div class="VideosView_results__YIV97"><div class="VideosView_result__7rgWv"><span>Input &amp; Coarse Masks</span><video id="result-video-controller" class="result-video" src="videos/Movies/chicken/matting-input.mp4" autoPlay="" loop="" controls="" muted=""></video></div></div></section><section class="VideosView_videoControls__2T6ai">Choose an experiment to view results. To control all videos:<button>Play</button><button>Pause</button><button>Sync wtih input</button></section><section class="VideosView_others___sJs1"><section class="VideosView_otherSelect__tpHde"><button class="VideosView_otherCurrent__wdxv_">Ours</button><button>Ours-Retrain BG</button><button>Omnimatte</button><button>Layered Neural Atlases</button></section><div class="VideosView_results__YIV97"><div class="VideosView_result__7rgWv"><span>Foreground 1</span><video class="result-video" src="videos/Movies/chicken/matting-fg0.mp4" autoPlay="" loop="" controls="" muted=""></video></div><div class="VideosView_result__7rgWv"><span>Foreground 2</span><video class="result-video" src="videos/Movies/chicken/matting-fg1.mp4" autoPlay="" loop="" controls="" muted=""></video></div><div class="VideosView_result__7rgWv"><span>Background RGB</span><video class="result-video" src="videos/Movies/chicken/matting-bg.mp4" autoPlay="" loop="" controls="" muted=""></video></div><div class="VideosView_result__7rgWv"><span>Background Depth</span><video class="result-video" src="videos/Movies/chicken/matting-depth.mp4" autoPlay="" loop="" controls="" muted=""></video></div></div></section></section><section><p></p><p>Download our <span class="dataset">Movies</span> and <span class="dataset">Wild</span> datasets here: <a target="_blank" href="https://drive.google.com/drive/folders/1PSEcqUR1prfQ51jzlCJ7tWDHPXmZGbMo?usp=sharing">Google Drive</a></p><p>The <span class="dataset">Movies</span> dataset contains 5 sequences from 3 Blender movies. They come with ground truth camera poses, object masks, and clean background videos.</p><p>Vidoes in <span class="dataset">Wild</span> are captured by us and come with reconstructed camera poses and coarse masks.</p><p>Other videos used in the paper are obtained from their authors: <a target="_blank" href="https://davischallenge.org/davis2017/code.html"><span class="dataset">DAVIS</span></a>, <a target="_blank" href="https://d2nerf.github.io/"><span class="dataset">Kubric</span></a>, <a target="_blank" href="https://omnimatte.github.io/">dogwalk</a></p></section></section><section><h2>Presentation Video</h2><p>Coming soon!</p></section><section class="component_bibtex__pEVQH"><h2>BibTeX</h2><button id="copy-bibtex-btn">Copy to clipboard</button><pre>

@InProceedings{Lin_2023_ICCV,
  author    = {Geng Lin and Chen Gao and Jia-Bin Huang and Changil Kim and Yipeng Wang and Matthias Zwicker and Ayush Saraf},
  title     = {OmnimatteRF: Robust Omnimatte with 3D Background Modeling},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2023}
}</pre></section></main><script src="/_next/static/chunks/webpack-2dcfb94f7fcf1987.js" async=""></script><script src="/_next/static/chunks/bce60fc1-fe2fbaac5e8de824.js" async=""></script><script src="/_next/static/chunks/769-174e81ca6f91fa79.js" async=""></script><script src="/_next/static/chunks/main-app-e20b864135fc14e9.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/e7c7dbb62ddcf6fa-s.p.woff2\",{\"as\":\"font\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/4dc903766d1a074c.css\",{\"as\":\"style\"}]\n0:\"$L3\"\n"])</script><script>self.__next_f.push([1,"4:HL[\"/_next/static/css/5baff23b1771b846.css\",{\"as\":\"style\"}]\n"])</script><script>self.__next_f.push([1,"5:I{\"id\":\"8802\",\"chunks\":[\"272:static/chunks/webpack-2dcfb94f7fcf1987.js\",\"253:static/chunks/bce60fc1-fe2fbaac5e8de824.js\",\"769:static/chunks/769-174e81ca6f91fa79.js\"],\"name\":\"\",\"async\":false}\n7:I{\"id\":\"4299\",\"chunks\":[\"272:static/chunks/webpack-2dcfb94f7fcf1987.js\",\"253:static/chunks/bce60fc1-fe2fbaac5e8de824.js\",\"769:static/chunks/769-174e81ca6f91fa79.js\"],\"name\":\"\",\"async\":false}\n8:I{\"id\":\"7477\",\"chunks\":[\"625:static/chunks/625-af5b057c6fb50add.js\",\"185:static/chunks/app/layout-688dcdb02627854f.js\"],\"nam"])</script><script>self.__next_f.push([1,"e\":\"\",\"async\":false}\n9:I{\"id\":\"7682\",\"chunks\":[\"625:static/chunks/625-af5b057c6fb50add.js\",\"185:static/chunks/app/layout-688dcdb02627854f.js\"],\"name\":\"\",\"async\":false}\nb:I{\"id\":\"3211\",\"chunks\":[\"272:static/chunks/webpack-2dcfb94f7fcf1987.js\",\"253:static/chunks/bce60fc1-fe2fbaac5e8de824.js\",\"769:static/chunks/769-174e81ca6f91fa79.js\"],\"name\":\"\",\"async\":false}\nc:I{\"id\":\"5767\",\"chunks\":[\"272:static/chunks/webpack-2dcfb94f7fcf1987.js\",\"253:static/chunks/bce60fc1-fe2fbaac5e8de824.js\",\"769:static/chunks/769-174e8"])</script><script>self.__next_f.push([1,"1ca6f91fa79.js\"],\"name\":\"\",\"async\":false}\nd:I{\"id\":\"7218\",\"chunks\":[\"594:static/chunks/594-a265ff1a849d514a.js\",\"931:static/chunks/app/page-536da1bffc24002d.js\"],\"name\":\"\",\"async\":false}\ne:I{\"id\":\"7668\",\"chunks\":[\"594:static/chunks/594-a265ff1a849d514a.js\",\"931:static/chunks/app/page-536da1bffc24002d.js\"],\"name\":\"\",\"async\":false}\nf:I{\"id\":\"4222\",\"chunks\":[\"594:static/chunks/594-a265ff1a849d514a.js\",\"931:static/chunks/app/page-536da1bffc24002d.js\"],\"name\":\"\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"3:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/4dc903766d1a074c.css\",\"precedence\":\"next\"}]],[\"$\",\"$L5\",null,{\"buildId\":\"7QmDl34DUvUd4vXinwmEZ\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/\",\"initialTree\":[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],\"initialHead\":[\"$L6\",[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\"}]],\"globalErrorComponent\":\"$7\",\"notFound\":[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_f9cc9d\",\"children\":[\"$\",\"main\",null,{\"children\":[[[\"$\",\"section\",null,{\"className\":\"component_title__UkgDO\",\"children\":[[\"$\",\"h1\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"OmnimatteRF\"}],[\"$\",\"br\",null,{}],\"Robust Omnimatte with 3D Background Modeling\"]}],[\"$\",\"span\",null,{\"className\":\"component_published__rjSpb\",\"children\":\"ICCV 2023\"}]]}],[\"$\",\"section\",null,{\"className\":\"component_authorList__sLmCk\",\"children\":[[\"$\",\"span\",\"Geng Lin\",{\"className\":\"$undefined\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://scholar.google.com/citations?user=2Vh_sboAAAAJ\u0026hl=en\",\"target\":\"_blank\",\"children\":\"Geng Lin\"}],[\"$\",\"sup\",null,{\"children\":\"1\"}]]}],[\"$\",\"span\",\"Chen Gao\",{\"className\":\"$undefined\",\"children\":[[\"$\",\"a\",null,{\"href\":\"http://chengao.vision/\",\"target\":\"_blank\",\"children\":\"Chen Gao\"}],[\"$\",\"sup\",null,{\"children\":\"2\"}]]}],[\"$\",\"span\",\"Jia-Bin Huang\",{\"className\":\"$undefined\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://jbhuang0604.github.io/\",\"target\":\"_blank\",\"children\":\"Jia-Bin Huang\"}],[\"$\",\"sup\",null,{\"children\":\"1,2\"}]]}],[\"$\",\"span\",\"Changil Kim\",{\"className\":\"$undefined\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://changilkim.com/\",\"target\":\"_blank\",\"children\":\"Changil Kim\"}],[\"$\",\"sup\",null,{\"children\":\"2\"}]]}],[\"$\",\"span\",\"Yipeng Wang\",{\"className\":\"$undefined\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://www.linkedin.com/in/yipeng-wang99/\",\"target\":\"_blank\",\"children\":\"Yipeng Wang\"}],[\"$\",\"sup\",null,{\"children\":\"2\"}]]}],[\"$\",\"span\",\"Matthias Zwicker\",{\"className\":\"$undefined\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://www.cs.umd.edu/~zwicker/\",\"target\":\"_blank\",\"children\":\"Matthias Zwicker\"}],[\"$\",\"sup\",null,{\"children\":\"1\"}]]}],[\"$\",\"span\",\"Ayush Saraf\",{\"className\":\"$undefined\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://scholar.google.com/citations?user=bluhHm8AAAAJ\u0026hl=en\",\"target\":\"_blank\",\"children\":\"Ayush Saraf\"}],[\"$\",\"sup\",null,{\"children\":\"2\"}]]}]]}],[\"$\",\"section\",null,{\"className\":\"component_affiliationList__i5VyJ\",\"children\":[[\"$\",\"span\",\"University of Maryland, College Park\",{\"className\":\"$undefined\",\"children\":[[\"$\",\"sup\",null,{\"children\":1}],\"University of Maryland, College Park\"]}],[\"$\",\"span\",\"Meta\",{\"className\":\"$undefined\",\"children\":[[\"$\",\"sup\",null,{\"children\":2}],\"Meta\"]}]]}]],[\"$\",\"section\",null,{\"className\":\"component_links__O6lLr\",\"children\":[[\"$\",\"$L8\",null,{\"className\":\"component_link__LvP1v\",\"target\":\"_blank\",\"href\":\"https://arxiv.org/abs/2309.07749\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"fill\":\"none\",\"viewBox\":\"0 0 24 24\",\"strokeWidth\":1.5,\"stroke\":\"currentColor\",\"aria-hidden\":\"true\",\"aria-labelledby\":\"$undefined\",\"children\":[null,[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"d\":\"M19.5 14.25v-2.625a3.375 3.375 0 00-3.375-3.375h-1.5A1.125 1.125 0 0113.5 7.125v-1.5a3.375 3.375 0 00-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 00-9-9z\"}]]}],\" Paper\"]}],[\"$\",\"$L8\",null,{\"className\":\"component_link__LvP1v\",\"target\":\"_blank\",\"href\":\"https://github.com/facebookresearch/OmnimatteRF\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"fill\":\"none\",\"viewBox\":\"0 0 24 24\",\"strokeWidth\":1.5,\"stroke\":\"currentColor\",\"aria-hidden\":\"true\",\"aria-labelledby\":\"$undefined\",\"children\":[null,[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"d\":\"M17.25 6.75L22.5 12l-5.25 5.25m-10.5 0L1.5 12l5.25-5.25m7.5-3l-4.5 16.5\"}]]}],\" Code\"]}],[\"$\",\"$L9\",null,{}]]}],[\"$\",\"section\",null,{\"className\":\"empty\"}],[\"$La\",\"$undefined\",[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]]]]}]}]}],\"asNotFound\":false,\"children\":[[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_f9cc9d\",\"children\":[\"$\",\"main\",null,{\"children\":[[[\"$\",\"section\",null,{\"className\":\"component_title__UkgDO\",\"children\":[[\"$\",\"h1\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"OmnimatteRF\"}],[\"$\",\"br\",null,{}],\"Robust Omnimatte with 3D Background Modeling\"]}],[\"$\",\"span\",null,{\"className\":\"component_published__rjSpb\",\"children\":\"ICCV 2023\"}]]}],[\"$\",\"section\",null,{\"className\":\"component_authorList__sLmCk\",\"children\":[[\"$\",\"span\",\"Geng Lin\",{\"className\":\"$undefined\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://scholar.google.com/citations?user=2Vh_sboAAAAJ\u0026hl=en\",\"target\":\"_blank\",\"children\":\"Geng Lin\"}],[\"$\",\"sup\",null,{\"children\":\"1\"}]]}],[\"$\",\"span\",\"Chen Gao\",{\"className\":\"$undefined\",\"children\":[[\"$\",\"a\",null,{\"href\":\"http://chengao.vision/\",\"target\":\"_blank\",\"children\":\"Chen Gao\"}],[\"$\",\"sup\",null,{\"children\":\"2\"}]]}],[\"$\",\"span\",\"Jia-Bin Huang\",{\"className\":\"$undefined\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://jbhuang0604.github.io/\",\"target\":\"_blank\",\"children\":\"Jia-Bin Huang\"}],[\"$\",\"sup\",null,{\"children\":\"1,2\"}]]}],[\"$\",\"span\",\"Changil Kim\",{\"className\":\"$undefined\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://changilkim.com/\",\"target\":\"_blank\",\"children\":\"Changil Kim\"}],[\"$\",\"sup\",null,{\"children\":\"2\"}]]}],[\"$\",\"span\",\"Yipeng Wang\",{\"className\":\"$undefined\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://www.linkedin.com/in/yipeng-wang99/\",\"target\":\"_blank\",\"children\":\"Yipeng Wang\"}],[\"$\",\"sup\",null,{\"children\":\"2\"}]]}],[\"$\",\"span\",\"Matthias Zwicker\",{\"className\":\"$undefined\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://www.cs.umd.edu/~zwicker/\",\"target\":\"_blank\",\"children\":\"Matthias Zwicker\"}],[\"$\",\"sup\",null,{\"children\":\"1\"}]]}],[\"$\",\"span\",\"Ayush Saraf\",{\"className\":\"$undefined\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://scholar.google.com/citations?user=bluhHm8AAAAJ\u0026hl=en\",\"target\":\"_blank\",\"children\":\"Ayush Saraf\"}],[\"$\",\"sup\",null,{\"children\":\"2\"}]]}]]}],[\"$\",\"section\",null,{\"className\":\"component_affiliationList__i5VyJ\",\"children\":[[\"$\",\"span\",\"University of Maryland, College Park\",{\"className\":\"$undefined\",\"children\":[[\"$\",\"sup\",null,{\"children\":1}],\"University of Maryland, College Park\"]}],[\"$\",\"span\",\"Meta\",{\"className\":\"$undefined\",\"children\":[[\"$\",\"sup\",null,{\"children\":2}],\"Meta\"]}]]}]],[\"$\",\"section\",null,{\"className\":\"component_links__O6lLr\",\"children\":[[\"$\",\"$L8\",null,{\"className\":\"component_link__LvP1v\",\"target\":\"_blank\",\"href\":\"https://arxiv.org/abs/2309.07749\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"fill\":\"none\",\"viewBox\":\"0 0 24 24\",\"strokeWidth\":1.5,\"stroke\":\"currentColor\",\"aria-hidden\":\"true\",\"aria-labelledby\":\"$undefined\",\"children\":[null,[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"d\":\"M19.5 14.25v-2.625a3.375 3.375 0 00-3.375-3.375h-1.5A1.125 1.125 0 0113.5 7.125v-1.5a3.375 3.375 0 00-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 00-9-9z\"}]]}],\" Paper\"]}],[\"$\",\"$L8\",null,{\"className\":\"component_link__LvP1v\",\"target\":\"_blank\",\"href\":\"https://github.com/facebookresearch/OmnimatteRF\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"fill\":\"none\",\"viewBox\":\"0 0 24 24\",\"strokeWidth\":1.5,\"stroke\":\"currentColor\",\"aria-hidden\":\"true\",\"aria-labelledby\":\"$undefined\",\"children\":[null,[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"d\":\"M17.25 6.75L22.5 12l-5.25 5.25m-10.5 0L1.5 12l5.25-5.25m7.5-3l-4.5 16.5\"}]]}],\" Code\"]}],[\"$\",\"$L9\",null,{}]]}],[\"$\",\"section\",null,{\"className\":\"empty\"}],[\"$\",\"$Lb\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"template\":[\"$\",\"$Lc\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[[[\"$\",\"section\",null,{\"className\":\"$undefined\",\"children\":[\"$\",\"center\",null,{\"children\":[\"$\",\"video\",null,{\"src\":\"/matting-teaser.mp4\",\"autoPlay\":true,\"loop\":true,\"muted\":true}]}]}],[\"$\",\"$Ld\",null,{}],[\"$\",\"section\",null,{\"className\":\"$undefined\",\"children\":[[\"$\",\"h2\",null,{\"children\":\"Abstract\"}],[\"$\",\"p\",null,{\"children\":\" Video matting has broad applications, from adding interesting effects to casually captured movies to assisting video production professionals. Matting with associated effects like shadows and reflections has also attracted increasing research activity, and methods like Omnimatte have been proposed to separate foreground objects of interest into their own layers. However, prior works represent video backgrounds as 2D image layers, limiting their capacity to express more complicated scenes, thus hindering application to real-world videos. In this paper, we propose a novel video matting method, F2B3, that combines 2D foreground layers and a 3D background model. The 2D layers preserve the details of the subjects, while the 3D background robustly reconstructs scenes in real-world videos. Extensive experiments demonstrate that our method reconstructs with better quality on various videos.\"}]]}],[\"$\",\"section\",null,{\"className\":\"component_method__8zQ7M\",\"children\":[[\"$\",\"h2\",null,{\"children\":\"Method\"}],[\"$\",\"center\",null,{\"children\":[\"$\",\"img\",null,{\"src\":\"/overview.jpg\"}]}],[\"$\",\"p\",null,{}],[\"$\",\"p\",null,{\"children\":[\"OmnimatteRF extends \",[\"$\",\"i\",null,{\"children\":\"Omnimatting\"}],\" to a larger variety of real-world videos with a combination of 2D foreground layers and a background radiance field.\"]}]]}],[\"$\",\"section\",null,{\"className\":\"component_datasets__fjKYl\",\"children\":[[\"$\",\"h2\",null,{\"id\":\"data\",\"children\":\"Data \u0026 Results\"}],[\"$\",\"$Le\",null,{}],[\"$\",\"section\",null,{\"children\":[[\"$\",\"p\",null,{}],[\"$\",\"p\",null,{\"children\":[\"Download our \",[\"$\",\"span\",null,{\"className\":\"dataset\",\"children\":\"Movies\"}],\" and \",[\"$\",\"span\",null,{\"className\":\"dataset\",\"children\":\"Wild\"}],\" datasets here: \",[\"$\",\"a\",null,{\"target\":\"_blank\",\"href\":\"https://drive.google.com/drive/folders/1PSEcqUR1prfQ51jzlCJ7tWDHPXmZGbMo?usp=sharing\",\"children\":\"Google Drive\"}]]}],[\"$\",\"p\",null,{\"children\":[\"The \",[\"$\",\"span\",null,{\"className\":\"dataset\",\"children\":\"Movies\"}],\" dataset contains 5 sequences from 3 Blender movies. They come with ground truth camera poses, object masks, and clean background videos.\"]}],[\"$\",\"p\",null,{\"children\":[\"Vidoes in \",[\"$\",\"span\",null,{\"className\":\"dataset\",\"children\":\"Wild\"}],\" are captured by us and come with reconstructed camera poses and coarse masks.\"]}],[\"$\",\"p\",null,{\"children\":[\"Other videos used in the paper are obtained from their authors: \",[\"$\",\"a\",null,{\"target\":\"_blank\",\"href\":\"https://davischallenge.org/davis2017/code.html\",\"children\":[\"$\",\"span\",null,{\"className\":\"dataset\",\"children\":\"DAVIS\"}]}],\", \",[\"$\",\"a\",null,{\"target\":\"_blank\",\"href\":\"https://d2nerf.github.io/\",\"children\":[\"$\",\"span\",null,{\"className\":\"dataset\",\"children\":\"Kubric\"}]}],\", \",[\"$\",\"a\",null,{\"target\":\"_blank\",\"href\":\"https://omnimatte.github.io/\",\"children\":\"dogwalk\"}]]}]]}]]}],[\"$\",\"section\",null,{\"className\":\"$undefined\",\"children\":[[\"$\",\"h2\",null,{\"children\":\"Presentation Video\"}],[\"$\",\"p\",null,{\"children\":\"Coming soon!\"}]]}],[\"$\",\"$Lf\",null,{}]],null],\"segment\":\"__PAGE__\"},\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/5baff23b1771b846.css\",\"precedence\":\"next\"}]]}]]}]}]}],null]}]]\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n6:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"OmnimatteRF: Robust Omnimatte with 3D Background Modeling\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"OmnimatteRF: Robust Omnimatte with 3D Background Modeling\"}],[\"$\",\"meta\",\"3\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script></body></html>